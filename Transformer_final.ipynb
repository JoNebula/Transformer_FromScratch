{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "sbGWaGRMqrir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext\n",
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMb1vaOFZd3K",
        "outputId": "d0cd066e-995f-4ad2-ec73-f52acbe2f40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (0.13.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.0->torchtext) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.8/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.12.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.0->torchdata) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVBFwuZUlN3i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import importlib\n",
        "\n",
        "from datetime import datetime as dt\n",
        "import time\n",
        "\n",
        "import imdb_voc\n",
        "import torchdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = './'\n",
        "\n",
        "# import sentences\n",
        "importlib.reload(imdb_voc)\n",
        "\n",
        "# set device\n",
        "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "pF43XCHtlVzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "vlHWQJQHqwpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_Q, d_K, d_V, numhead, dropout):\n",
        "      super().__init__()\n",
        "\n",
        "      # Q1. Implement\n",
        "\n",
        "      self.numhead=numhead\n",
        "      self.d_V = d_V\n",
        "      self.d_K = d_K\n",
        "      self.d_Q = d_Q\n",
        "      # input linear layers for V, Q, K\n",
        "      # d_Q, d_K, d_V are typically set to d_model/numhead\n",
        "\n",
        "      self.V_Linear = nn.Linear(in_features=d_model, out_features=d_V*numhead)\n",
        "      self.Q_Linear = nn.Linear(in_features=d_model, out_features=d_K*numhead)\n",
        "      self.K_Linear = nn.Linear(in_features=d_model, out_features=d_Q*numhead)\n",
        "\n",
        "      # output linear layer\n",
        "      self.MHA_Linear = nn.Linear(in_features=d_V*numhead, out_features=d_model)\n",
        "\n",
        "      # dropout\n",
        "      self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_Q, x_K, x_V, src_batch_lens=None):\n",
        "      # Q2 Implementation\n",
        "\n",
        "      batch_size = x_Q.size(0)\n",
        "      out = []\n",
        "      for i in range(batch_size):\n",
        "          q = self.Q_Linear(x_Q[i]).transpose(0, 1)\n",
        "          k = self.K_Linear(x_K[i]).transpose(0, 1)\n",
        "          v = self.V_Linear(x_V[i]).transpose(0, 1)\n",
        "          dot = torch.matmul(q, k.transpose(0, 1))\n",
        "          dot = dot / math.sqrt(dot.size(0))\n",
        "          att = F.softmax(dot, dim=1)\n",
        "          att = self.dropout(att)\n",
        "          out.append(torch.matmul(att, v))\n",
        "\n",
        "      # Stack the output tensors together\n",
        "      out = torch.stack(out)\n",
        "\n",
        "      mask = torch.arange(out.size(1)).unsqueeze(0).expand(batch_size, -1).to(x_Q.device)\n",
        "      mask = mask < src_batch_lens.unsqueeze(1)\n",
        "      mask = mask.unsqueeze(2)  # Expand the mask along the third dimension (numhead * d_V)\n",
        "\n",
        "      # Apply the mask to the output tensors\n",
        "      out = out.masked_fill(~mask, 0)\n",
        "\n",
        "      out = out.view(batch_size, -1, self.numhead * self.d_V)\n",
        "\n",
        "      out = self.MHA_Linear(out)\n",
        "      out = self.dropout(out)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "Mb6jKPrIu9rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TF_Encoder_Block(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, numhead, dropout):\n",
        "      super().__init__()\n",
        "\n",
        "      # Q3\n",
        "      self.MHA = MultiHeadAttention(d_model, d_model, d_model, d_model, numhead, dropout)\n",
        "      self.FF1 = nn.Linear(d_model, d_ff)\n",
        "      self.FF2 = nn.Linear(d_model, d_ff)\n",
        "      self.Dropout = nn.Dropout(dropout)\n",
        "      self.ReLU = nn.ReLU()\n",
        "      self.LayerNorm1 = nn.LayerNorm(d_model)\n",
        "      self.LayerNorm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_batch_lens):\n",
        "      # Q4\n",
        "      att = self.MHA(x, x, x, src_batch_lens)\n",
        "      x = x + att\n",
        "      x = self.LayerNorm1(x)\n",
        "      ff = self.FF1(x)\n",
        "      ff = self.ReLU(ff)\n",
        "      ff = self.Dropout(ff)\n",
        "      ff = self.FF2(ff)\n",
        "      ff = self.Dropout(ff)\n",
        "      x = x + ff\n",
        "      out = self.LayerNorm2(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "bE_oNT14poE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Positional encoding\n",
        "PE(pos,2i) = sin(pos/10000**(2i/dmodel))\n",
        "PE(pos,2i+1) = cos(pos/10000**(2i/dmodel))\n",
        "\"\"\"\n",
        "\n",
        "def PosEncoding(t_len, d_model):\n",
        "    i = torch.tensor(range(d_model))\n",
        "    pos = torch.tensor(range(t_len))\n",
        "    POS, I = torch.meshgrid(pos, i)\n",
        "    PE = (1-I % 2)*torch.sin(POS/10**(4*I/d_model)) + (I%2)*torch.cos(POS/10**(4*(I-1)/d_model))\n",
        "    return PE\n",
        "\n",
        "class TF_Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model,\n",
        "                 d_ff, numlayer, numhead, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.numlayer = numlayer\n",
        "        self.src_embed  = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        # Q5. Implement a sequence of numlayer encoder blocks\n",
        "        self.encoder_block = nn.ModuleList([TF_Encoder_Block(d_model, d_ff, numhead, dropout) for _ in range(numlayer)])\n",
        "\n",
        "    def forward(self, x, src_batch_lens):\n",
        "\n",
        "      x_embed = self.src_embed(x)\n",
        "      x = self.dropout(x_embed)\n",
        "      p_enc = PosEncoding(x.shape[1], x.shape[2]).to(dev)\n",
        "      x = x + p_enc\n",
        "      # Q6. Implement: forward over numlayer encoder blocks\n",
        "      for layer in self.encoder_block:\n",
        "        x = layer(x, src_batch_lens)\n",
        "        out = x\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "WjzgQewUljBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "main model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class sentiment_classifier(nn.Module):\n",
        "\n",
        "    def __init__(self, enc_input_size,\n",
        "                 enc_d_model,\n",
        "                 enc_d_ff,\n",
        "                 enc_num_layer,\n",
        "                 enc_num_head,\n",
        "                 dropout,\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = TF_Encoder(vocab_size = enc_input_size,\n",
        "                                  d_model = enc_d_model, d_ff=enc_d_ff,\n",
        "                                  numlayer=enc_num_layer, numhead=enc_num_head,\n",
        "                                  dropout=dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,None)),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features = enc_d_model, out_features=enc_d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features = enc_d_model, out_features = 1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        src_ctx = self.encoder(x, src_batch_lens = x_lens)\n",
        "        out_logits = self.classifier(src_ctx).flatten()\n",
        "        return out_logits"
      ],
      "metadata": {
        "id": "4lg0jt1mlqpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "datasets\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Load IMDB dataset\n",
        "# once you build the dataset, you can load it from file to save time\n",
        "# to load from file, set this flag True\n",
        "load_imdb_dataset = True\n",
        "\n",
        "if load_imdb_dataset:\n",
        "    imdb_dataset = torch.load('imdb_dataset.pt')\n",
        "else:\n",
        "    imdb_dataset = imdb_voc.IMDB_tensor_dataset()\n",
        "    torch.save(imdb_dataset, 'imdb_dataset.pt')"
      ],
      "metadata": {
        "id": "m6mZpRgXltdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = imdb_dataset.get_dataset()\n",
        "\n",
        "split_ratio = 0.85\n",
        "num_train = int(len(train_dataset) * split_ratio)\n",
        "split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "batch_size_trn = 64 # Hyperparam\n",
        "batch_size_val = 64\n",
        "batch_size_tst = 256\n",
        "\n",
        "train_dataloader = DataLoader(split_train, batch_size=batch_size_trn, shuffle=True)\n",
        "val_dataloader = DataLoader(split_valid, batch_size=batch_size_val, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size_tst, shuffle=True)\n",
        "\n",
        "# get character dictionary\n",
        "src_word_dict = imdb_dataset.src_stoi\n",
        "src_idx_dict = imdb_dataset.src_itos\n",
        "\n",
        "SRC_PAD_IDX = src_word_dict['<PAD>']"
      ],
      "metadata": {
        "id": "C589KmAelxGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show sample reviews with pos/neg sentiments\n",
        "\n",
        "show_sample_reviews = True\n",
        "\n",
        "if show_sample_reviews:\n",
        "\n",
        "    sample_text, sample_lab = next(iter(train_dataloader))\n",
        "    slist=[]\n",
        "\n",
        "    for stxt in sample_text[:4]:\n",
        "        slist.append([src_idx_dict[j] for j in stxt])\n",
        "\n",
        "    for j, s in enumerate(slist):\n",
        "        print('positive' if sample_lab[j]==1 else 'negative')\n",
        "        print(' '.join([i for i in s if i != '<PAD>'])+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA1FtrM3l0wg",
        "outputId": "ac4a0f9b-0890-447b-eff6-be4620af4a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n",
            "a major disappointment . this was one of the best uk crime drama / detective shows from the 90 ' s which developed the fascinating title character played by scotland ' s robbie coltrane . however this <UNK> has little to add and perhaps suffers from an inevitable let down due to raised expectations when a favored show returns after a long hiatus . coltrane isn ' t really given much to do , much more attention is spent on the uninteresting killer , and in what he has to act in , he seems <UNK> , almost bored . the <UNK> ' s story is written by the books and the attempt to update us on coltrane ' s family life seems lightweight . perhaps if the writers had a whole series in front of them instead of just this one two-hour show they would have written this with much more depth . as is , skip this and watch the old cracker from the 90 ' s which is far far superior .\n",
            "\n",
            "negative\n",
            "<UNK> ( laurel <UNK> ) becomes a live in babysitter for young <UNK> <UNK> ( <UNK> cole ) who has recently lost her mother . but <UNK> misses her dead mother a lot and continuously visits her grave ( conveniently located in a cemetery right behind the house ) late at night . . . where she also meets her friends . . . this starts off good with a truly eerie sequence in the cemetery . . . then falls apart . the story is thin and there is tons of padding to make the film 85 minutes long . the acting is terrible across the board ( with cole easily being the worst ) . badly directed with some of the worst editing i ' ve ever seen in a motion picture . scenes ( and sound ) are just cut off with no rhyme or reason . also the film has terrible ( and obvious ) post-production sound . as for blood and <UNK> it ! there ' s very little and what there is looks incredibly fake . i ' ve never seen such <UNK> <UNK> like ketchup ! boring , <UNK> rightfully forgotten drive-in movie . you can skip this one .\n",
            "\n",
            "positive\n",
            "first off , i ' d like to make a correction on another review of this film which said that the last musical to win the best picture academy award was ' gigi ' in 1958 . that is <UNK> as ' west side story ' won in 1962 , ' my fair lady ' won in 1965 and ' sound of music ' won the year after that . that said , this film is absolutely fantastic ! the story from the novel has been somewhat altered , but that ' s more because of the limitations that they had on a stage that they just didn ' t change back for a filmed version . however , i don ' t mind . in fact , i rather think the whole production flows better than the novel does . i like nancy bringing oliver to the bridge with her and being killed there instead of later in the apartment . the subtle things in this film are the ones that make me laugh . i love the moment mr . bumble and mrs . bumble start coming out at the beginning from the <UNK> office . the <UNK> for that moment is brilliant . my three favorite actors for this film were jack wild , who plays the best dodger in any film version of the story , ron moody , a playful and humorous fagin ( this character is worked out much better than he is in the book ) , and <UNK> <UNK> , who is the strongest , most distressed version of nancy . the only reason i ' m giving this film a 9 instead of a 10 is because of the two big production numbers which are ' <UNK> yourself ' and ' who will buy . ' i always hate when the choreography in musicals in meant to look like people doing everyday chores and jobs . it looks awful and cheesy , especially when they ' re dancing at the london meat co . they should ' ve just done regular choreography for these scenes . however , this film is a rare treasure that will stay with us , hopefully , forever .\n",
            "\n",
            "negative\n",
            "the film begins with people on earth discovering that their rocket to mars had not been lost but was just drifting out in space near out planet . when it ' s <UNK> , one of the crew members is ill , one is alive and the other two are missing . what happened to them is told through a flashback by the surviving member . while on mars , the crew was apparently attacked by a whole host of very silly <UNK> monsters . oddly , while the sets were pretty good , the monsters were among the silliest i have seen on film . plus , in an odd attempt at realism , the production used a process called <UNK> . unfortunately , this wonderful innovation just made the film look pretty cheap when they were on the surface of mars and the intensity of the <UNK> practically made my eyes <UNK> was that bad ! ! despite all the cheese , the film did have a somewhat interesting plot as well as a good message about space travel . for lovers of the genre , it ' s well worth seeing . for others , you may just find the whole thing rather <UNK> for yourself and decide . while by today ' s standards this isn ' t an especially good sci-fi film , compared with the films being made at the time , it <UNK> up pretty well . <UNK> you watch the film , pay careful attention to dr . <UNK> . he looks like the spitting image of dr . quest from the jonny quest cartoon ! plus , he sounds and acts a lot like him , too .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "enc_vocab_size = len(src_word_dict) # counting eof, one-hot vector goes in\n",
        "\n",
        "# Set hyperparam (model size)\n",
        "# examples: model & ff dim - 8, 16, 32, 64, 128, numhead, numlayer 1~4\n",
        "\n",
        "enc_d_model = 8 # Hyperparam\n",
        "enc_d_ff = 8 # Hyperparam\n",
        "enc_num_head = 4 # Hyperparam\n",
        "enc_num_layer= 4 # Hyperparam\n",
        "\n",
        "DROPOUT=0.1\n",
        "\n",
        "model = sentiment_classifier(enc_input_size=enc_vocab_size,\n",
        "                         enc_d_model = enc_d_model,\n",
        "                         enc_d_ff = enc_d_ff,\n",
        "                         enc_num_head = enc_num_head,\n",
        "                         enc_num_layer = enc_num_layer,\n",
        "                         dropout=DROPOUT)\n",
        "\n",
        "model = model.to(dev)"
      ],
      "metadata": {
        "id": "lY9J9huAni1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "optimizer\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Set hyperparam (learning rate)\n",
        "# examples: 1e-3 ~ 1e-5\n",
        "\n",
        "lr = 0.001 # Hyperparam\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "auxiliary functions\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# get length of reviews in batch\n",
        "def get_lens_from_tensor(x):\n",
        "    # lens (batch, t)\n",
        "    lens = torch.ones_like(x).long()\n",
        "    lens[x==SRC_PAD_IDX]=0\n",
        "    return torch.sum(lens, dim=-1)\n",
        "\n",
        "def get_binary_metrics(y_pred, y):\n",
        "    # find number of TP, TN, FP, FN\n",
        "    TP=sum(((y_pred == 1)&(y==1)).type(torch.int32))\n",
        "    FP=sum(((y_pred == 1)&(y==0)).type(torch.int32))\n",
        "    TN=sum(((y_pred == 0)&(y==0)).type(torch.int32))\n",
        "    FN=sum(((y_pred == 0)&(y==1)).type(torch.int32))\n",
        "    accy = (TP+TN)/(TP+FP+TN+FN)\n",
        "\n",
        "    recall = TP/(TP+FN) if TP+FN!=0 else 0\n",
        "    prec = TP/(TP+FP) if TP+FP!=0 else 0\n",
        "    f1 = 2*recall*prec/(recall+prec) if recall+prec !=0 else 0\n",
        "    return accy, recall, prec, f1"
      ],
      "metadata": {
        "id": "X_WyZ1p5nl6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "_Vz0DGddq03y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "train/validation\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        src = batch[0].to(dev)\n",
        "        trg = batch[1].float().to(dev)\n",
        "\n",
        "        # print('batch trg.shape', trg.shape)\n",
        "        # print('batch src.shape', src.shape)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_lens = get_lens_from_tensor(src).to(dev)\n",
        "\n",
        "        output = model(x=src, x_lens=x_lens)\n",
        "\n",
        "\n",
        "        output = output.contiguous().view(-1)\n",
        "        trg = trg.contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "D-wKDC0Knpc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    epoch_accy =0\n",
        "    epoch_recall =0\n",
        "    epoch_prec =0\n",
        "    epoch_f1 =0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "\n",
        "            src = batch[0].to(dev)\n",
        "            trg = batch[1].float().to(dev)\n",
        "\n",
        "            x_lens = get_lens_from_tensor(src).to(dev)\n",
        "\n",
        "            output = model(x=src, x_lens=x_lens)\n",
        "\n",
        "            output = output.contiguous().view(-1)\n",
        "            trg = trg.contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            accy, recall, prec, f1 = get_binary_metrics((output>=0).long(), trg.long())\n",
        "            epoch_accy += accy\n",
        "            epoch_recall += recall\n",
        "            epoch_prec += prec\n",
        "            epoch_f1 += f1\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    # show accuracy\n",
        "    print(f'\\tAccuracy: {epoch_accy/(len(dataloader)):.3f}')\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "-cFPt8tonsYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Training loop\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "N_EPOCHS = 30\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    #here\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_dataloader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmDjciTXnvDD",
        "outputId": "02b367ae-8065-4ea9-aa31-884fc8f74135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tAccuracy: 0.586\n",
            "Epoch: 01 | Time: 0m 56s\n",
            "\tTrain Loss: 0.689 | Val. Loss: 0.670\n",
            "\tAccuracy: 0.706\n",
            "Epoch: 02 | Time: 0m 55s\n",
            "\tTrain Loss: 0.632 | Val. Loss: 0.574\n",
            "\tAccuracy: 0.750\n",
            "Epoch: 03 | Time: 0m 58s\n",
            "\tTrain Loss: 0.567 | Val. Loss: 0.509\n",
            "\tAccuracy: 0.785\n",
            "Epoch: 04 | Time: 0m 58s\n",
            "\tTrain Loss: 0.512 | Val. Loss: 0.464\n",
            "\tAccuracy: 0.812\n",
            "Epoch: 05 | Time: 0m 56s\n",
            "\tTrain Loss: 0.465 | Val. Loss: 0.424\n",
            "\tAccuracy: 0.821\n",
            "Epoch: 06 | Time: 0m 56s\n",
            "\tTrain Loss: 0.430 | Val. Loss: 0.406\n",
            "\tAccuracy: 0.833\n",
            "Epoch: 07 | Time: 0m 57s\n",
            "\tTrain Loss: 0.400 | Val. Loss: 0.396\n",
            "\tAccuracy: 0.846\n",
            "Epoch: 08 | Time: 0m 55s\n",
            "\tTrain Loss: 0.375 | Val. Loss: 0.361\n",
            "\tAccuracy: 0.849\n",
            "Epoch: 09 | Time: 0m 54s\n",
            "\tTrain Loss: 0.352 | Val. Loss: 0.373\n",
            "\tAccuracy: 0.857\n",
            "Epoch: 10 | Time: 0m 56s\n",
            "\tTrain Loss: 0.337 | Val. Loss: 0.349\n",
            "\tAccuracy: 0.862\n",
            "Epoch: 11 | Time: 0m 54s\n",
            "\tTrain Loss: 0.323 | Val. Loss: 0.348\n",
            "\tAccuracy: 0.865\n",
            "Epoch: 12 | Time: 0m 55s\n",
            "\tTrain Loss: 0.304 | Val. Loss: 0.348\n",
            "\tAccuracy: 0.862\n",
            "Epoch: 13 | Time: 0m 54s\n",
            "\tTrain Loss: 0.289 | Val. Loss: 0.349\n",
            "\tAccuracy: 0.869\n",
            "Epoch: 14 | Time: 0m 56s\n",
            "\tTrain Loss: 0.278 | Val. Loss: 0.351\n",
            "\tAccuracy: 0.868\n",
            "Epoch: 15 | Time: 0m 55s\n",
            "\tTrain Loss: 0.270 | Val. Loss: 0.344\n",
            "\tAccuracy: 0.867\n",
            "Epoch: 16 | Time: 0m 55s\n",
            "\tTrain Loss: 0.251 | Val. Loss: 0.346\n",
            "\tAccuracy: 0.853\n",
            "Epoch: 17 | Time: 0m 56s\n",
            "\tTrain Loss: 0.245 | Val. Loss: 0.378\n",
            "\tAccuracy: 0.879\n",
            "Epoch: 18 | Time: 0m 55s\n",
            "\tTrain Loss: 0.231 | Val. Loss: 0.326\n",
            "\tAccuracy: 0.870\n",
            "Epoch: 19 | Time: 0m 55s\n",
            "\tTrain Loss: 0.221 | Val. Loss: 0.351\n",
            "\tAccuracy: 0.876\n",
            "Epoch: 20 | Time: 0m 55s\n",
            "\tTrain Loss: 0.214 | Val. Loss: 0.343\n",
            "\tAccuracy: 0.871\n",
            "Epoch: 21 | Time: 0m 56s\n",
            "\tTrain Loss: 0.202 | Val. Loss: 0.369\n",
            "\tAccuracy: 0.879\n",
            "Epoch: 22 | Time: 0m 55s\n",
            "\tTrain Loss: 0.199 | Val. Loss: 0.372\n",
            "\tAccuracy: 0.876\n",
            "Epoch: 23 | Time: 0m 54s\n",
            "\tTrain Loss: 0.183 | Val. Loss: 0.355\n",
            "\tAccuracy: 0.871\n",
            "Epoch: 24 | Time: 0m 55s\n",
            "\tTrain Loss: 0.180 | Val. Loss: 0.377\n",
            "\tAccuracy: 0.878\n",
            "Epoch: 25 | Time: 0m 56s\n",
            "\tTrain Loss: 0.172 | Val. Loss: 0.366\n",
            "\tAccuracy: 0.877\n",
            "Epoch: 26 | Time: 0m 54s\n",
            "\tTrain Loss: 0.164 | Val. Loss: 0.374\n",
            "\tAccuracy: 0.876\n",
            "Epoch: 27 | Time: 0m 54s\n",
            "\tTrain Loss: 0.159 | Val. Loss: 0.393\n",
            "\tAccuracy: 0.877\n",
            "Epoch: 28 | Time: 0m 57s\n",
            "\tTrain Loss: 0.154 | Val. Loss: 0.374\n",
            "\tAccuracy: 0.877\n",
            "Epoch: 29 | Time: 0m 55s\n",
            "\tTrain Loss: 0.150 | Val. Loss: 0.392\n",
            "\tAccuracy: 0.874\n",
            "Epoch: 30 | Time: 0m 55s\n",
            "\tTrain Loss: 0.139 | Val. Loss: 0.442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "bbw4XBmvq3kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Test loop\n",
        "\n",
        "\"\"\"\n",
        "print('*** Now test phase begins! ***')\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, criterion)\n",
        "get_binary_metrics\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8kHTCaOoS3x",
        "outputId": "5af92e7b-3a31-469f-e798-aef688a6f8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Now test phase begins! ***\n",
            "\tAccuracy: 0.859\n",
            "| Test Loss: 0.357\n"
          ]
        }
      ]
    }
  ]
}